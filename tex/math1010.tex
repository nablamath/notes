% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
% CUHK Mathematics
% MATH1010: University Mathematics
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

% --------------------------------------------------------------
% Document Setup
% --------------------------------------------------------------

\documentclass[a4paper,12pt]{article}
\usepackage{standalone}
\input{sty/setup.sty}

% --------------------------------------------------------------
% Document Context
% --------------------------------------------------------------

\begin{document}
% Cover
\title{MATH1010 Notes}
\input{sty/cover.sty}

\n\n

\noindent \textbf{Remarks}
\begin{nlist}
  \item Context of this document is based on university course \textit{MATH1010: University Mathematics}, from \textit{Department of Mathematics, The Chinese University of Hong Kong (CUHK)}. The original source can be found at \url{https://www.math.cuhk.edu.hk/course}. The author does not own the source.
  \item This document is assumed unavailable for unauthorized parties that have not attended the university course. It is prohibited to share, including distributing or copying this document to unauthorized parties in any means for any non-academic purpose.
  \item Context of this doucment may not be completely accurate. The author assumes no responsibility or liability for any errors or omissions in the context of this document.
  \item This document is under license CC-BY-SA 4.0. It is allowed to make any editions on this document, as long as terms of the license is not violated.
  \color{zp}\item If you are a freshman to \textit{BSc in Mathematics, The Chinese University of Hong Kong (MATH)}, it is highly recommended to read the preface first. There may be useful information and resources for you to suit up yourself before getting started.
\end{nlist}

\noindent \textbf{Source}\n

The latest version of this document can be found at \url{https://www.github.com/onenylxus/math-notes}.

\pagebreak

% Header
\input{sty/header.sty}

% Context
\fancyhead[R]{Preface}
\section*{Preface}
\addcontentsline{toc}{section}{Preface}
I believe this is the beginning of your journey to university mathematics. Before we begin, please spend some time reading this preface.\n

It is assumed that you are familiar with the topics listed below, which in this course is considered as \textbf{school mathematics}:
\begin{alist}
  \item Surds
  \item **Quadratic Polynomials and Quadratic Equations
  \item Manipulation of Polynomials, Remainder Theorem and Factor Theorem
  \item Simultaneous Equations with Two Unknowns
  \item *Absolute Values
  \item Indices and Logarithms
  \item Ratios and Proportions
  \item **Algebraic Inequalities
  \item *Mathematical Induction
  \item *Binomial Theorem
  \item *Radian Measure
  \item **Trigonometric Functions
  \item *General Solutions of Trigonometric Equations
  \item *Solutions of Triangles
  \item *Vectors in $\R^{2}$ and $\R^{3}$
  \item *Complex Numbers (with Modulus and Arguments)
  \item **Plane Coordinate Geometry (for Lines and Circles)
  \item *Differentiation of Elementary Functions
  \item *Applications of Differentiation
  \item *Definite Integrals and Indefinite Integrals
  \item *Applications of Integration
\end{alist}
\pagebreak

Here are the further references about the list above:
\begin{Alist}
  \item Item Form IV-V in \href{https://www.edb.gov.hk/en/curriculum-development/kla/ma/curr/sec-math-1985.html}{\textit{Secondary Maths Syllabus 1985, Education Bureau}}
  \item Various items listed under the syllabus in \href{http://www.edb.gov.hk/en/curriculum-development/kla/ma/curr/add-math-1992.html}{\textit{Secondary Additional Maths Syllabus 1992, Education Bureau}}
\end{Alist}

Please refer to (B) for topics marked with one asterisk (*), and both (A) and (B) for topics marked with two asterisks (**). Other topics is relatively basic and can be referred to (A).\n

It is also encouraged to find the textbooks and treat the questions in the past examination papers of \textit{HKCEE Mathematics} and \textit{Additional Mathematics} as self-study and further revision exercises.\n

Here are some recommended books with each covering almost every topic listed above at an accessible level. They are all freely available in electronic form:
\begin{rlist}
  \item \href{https://archive.org/details/PureMathematicsForAdvancedLevel}{B. D. Bunday, H. Mulholland, \textit{Pure Mathematics for Advanced Level}, Butterworths 1970.}
  \item \href{https://archive.org/details/FurtherMathematics}{R. I. Porter, \textit{Further Mathematics}, Bell and Hyman Limited 1970.}
  \item \href{https://archive.org/details/in.ernet.dli.2015.285850}{C. J. Tranter, \textit{Advanced Level Pure Mathematics}, The English Universities Press.}
\end{rlist}

It is also highly recommended to attend the First Year Honours Scheme. The First Year Honours Scheme will help you prepare for the level-2000 courses, probably in the second year.\n

For more advice on study, please visit the website \url{http://www.math.cuhk.edu.hk/student-centre/academic-advice}.

\pagebreak

\fancyhead[R]{\nouppercase \lastrightmark}
\section{Limits}
\subsection{Introduction to Infinite Sequences}
\subsubsection{Infinite Sequences}
\begin{dft}
  An \textbf{infinite sequence of real numbers} is defined by a function from the set of positive integers $\Z^{+}=\brc{1\text{, }2\text{, }3\text{, ...}}$ to the set of real numbers $\R$.
\end{dft}

\subsubsection{Arithmetic Sequences}
\begin{dft}
  An \textbf{arithmetic sequence} is a sequence $a_{n}$ such that the difference of any two consecutive terms is constant. The constant is called the \textbf{common difference}, here denoted by $d$:

  $$a_{n+1}-a_{n}=d\erm{for any }n\in\Z^{+}$$
\end{dft}\n

The n-th term of an arithmetic sequence can be calculated by the following equation:

$$a_{n}=a_{1}+(n-1)d$$

\subsubsection{Geometric Sequences}
\begin{dft}
  A \textbf{geometric sequence} is a sequence $a_{n}$ such that the next term is the product of the previous term and a non-zero constant. The constant is called the \textbf{common ratio}, here denoted by $r$:

  $$\frac{a_{n+1}}{a_{n}}=r\erm{for any }n\in\Z^{+}$$
\end{dft}\n

The n-th term of a geometric sequence can be calculated by the following equation:

$$a_{n}=a_{1}r^{n-1}$$\s

Besides arithmetic sequence and geometric sequence, there are some interesting sequence such as the Fibonacci sequence.

\subsubsection{Fibonacci Sequence}
\begin{dft}
  The \textbf{Fibonacci sequence} is the sequence $F_{n}$ which satisfies the following:

  $$\begin{cases}
    F_{1}=F_{2}=1&\\
    F_{n+2}=F_{n+1}+F_{n}&\erm{for }n\leq 1
  \end{cases}$$
\end{dft}\n

Interestingly, the n-th term of the Fibonacci sequence can be calculated by the following equation:

$$F_{n}=\frac{1}{\sqrt{5}}\brr{\brr{\frac{1+\sqrt{5}}{2}}^{n}+\brr{\frac{1-\sqrt{5}}{2}}^{n}}$$

\subsection{Limits of Sequences}
\subsubsection{Convergence of Sequences}
\begin{dft}
  Denote an infinite sequence as $a_{n}$. Suppose there exists a real number L such that for any $\epsilon>0$, there exists some $N\in\N$ such that for any $n>\N$, $\abs{a_{n}-L}<\epsilon$. Then the infinite sequence is said to be \textbf{convergent}, or the infinite sequence \textbf{converges to} $L$:

  $$\lim_{n\to \infty}a_{n}=L$$\s

  Otherwise, infinite sequenceis said to be \textbf{divergent}, or the infinite sequence \textbf{diverges}.
\end{dft}

\subsubsection{Limits to Infinity}
\begin{dft}
  Denote an infinite sequence as $a_{n}$. Suppose for any $M>0$, there exists some $N\in\N$ such that for any $n>\N$, $a_{n}>M$. Then the infinite sequence \textbf{tends to} $+\infty$ as $n$ tends to infinity:

  $$\lim_{n\to \infty}a_{n}=+\infty$$
\end{dft}\n

An infinite sequence can also tend to $-\infty$ as $n$ tends to infinity, using similar method as above. Note that the infinite sequence is divergent if it tends to $\pm \infty$.

\subsubsection{Manipulations of Limits of Sequences}
\begin{pst}
  Here are some available manipulations of limits:

  \begin{alist}
    \item \textbf{Addition and Subtraction of Limits}\\
    Suppose $\lim_{n\to\infty}a_{n}=a$ and $\lim_{n\to\infty}b_{n}=b$. Then

    $$\lim_{n\to \infty}a_{n}\pm\lim_{n\to \infty}b_{n}=a\pm b$$

    \item \textbf{Scalar Multiplication of Limits}\\
    Suppose $\lim_{n\to\infty}a_{n}=a$ and $c$ is a real number. Then

    $$\lim_{n\to\infty}ca_{n}=ca$$

    \item \textbf{Multiplication of Limits}\\
    Suppose $\lim_{n\to\infty}a_{n}=a$ and $\lim_{n\to\infty}b_{n}=b$. Then

    $$\lim_{n\to\infty}a_{n}b_{n}=ab$$

    \item \textbf{Division of Limits}\\
    Suppose $\lim_{n\to\infty}a_{n}=a$ and $\lim_{n\to\infty}b_{n}=b\neq 0$. Then

    $$\lim_{n\to\infty}\frac{a_{n}}{b_{n}}=\frac{a}{b}$$
  \end{alist}
\end{pst}

\subsection{Monotonicity and Boundedness}
\subsubsection{Monotonic Sequences}
\begin{dft}
  Denote an infinite sequence as $a_{n}$. The infinite sequence is said to be \textbf{monotonic increasing (or decreasing)} if for any $m<n$, $a_{m}\leq a_{n}$ (or $a_{m}\geq a_{n}$). The infinite sequence is said to be \textbf{strictly increasing (or decreasing)} if for any $m<n$, $a_{m}<a_{n}$ (or $a_{m}>a_{n}$). The infinite sequence is also said to be \textbf{monotonic} if it is either monotonic increasing or monotonic decreasing.
\end{dft}

\subsubsection{Bounded Sequences}
\begin{dft}
  Denote an infinite sequence as $a_{n}$. The infinite sequence is said to be \textbf{bounded} if there exists real number $M$ such that $\abs{a_{n}}<M$ for any $n\in\Z^{+}$.
\end{dft}\n

If an infinite sequence is convergent, the infinite sequence is automatically bounded. However, it is not guaranteed for an bounded infinite sequence to be convergent.

\subsection{Theorems on Limits of Sequences}
\subsubsection{Monotone Convergence Theorem}
With the definition of boundedness and convergence of an infinite sequence, the \textbf{Monotone Convergence Theorem} can be introduced.\n

\begin{thm}
  If an infinite sequence is bounded and monotonic at the same time, then the infinite sequence is convergent.
\end{thm}\n

Here is the example of using the Monotone Convergence Theorem.\n

\begin{exm}
  Let $a_{n}$ be the infinite sequence defined by the recursive relation

  $$\begin{cases}
    a_{n+1}=\sqrt{a_{n}+1}&\erm{for any }n\geq 1\\
    a_{1}=1&
  \end{cases}$$\s

  Find $\lim_{n\to \infty}a_{n}$.\n

  \ans Note that it is not allowed to find the limit explicitly since the existence of limit is not guaranteed. For the existence of limit, Monotone Convergence Theorem has to be applied first, where boundedness and monotonicity of the infinite sequence has to be proved.\n

  \begin{alist}
   \item \textbf{Boundedness}\n

   The objective of boundedness is to prove that $1\leq a_{n}<2$ for all $n\geq 1$ by induction.

   \begin{rlist}
     \item When $n=1$, $a_{1}=1$ and $1\leq a_{1}<2$.

     \item Assume that $1\leq a_{k}<2$ for some $k\in\Z^{+}$. Then

     $$\begin{aligned}[t]
       a_{k+1}&=\sqrt{a_{k}+1}\geq\sqrt{1+1}>1\\
       a_{k+1}&=\sqrt{a_{k}+1}<\sqrt{2+1}<2
     \end{aligned}$$
   \end{rlist}

   Thus $1\leq a_{n}<2$ for any $n\geq 1$ which implies that $a_{n}$ is bounded.

   \item \textbf{Monotonicity}\n

   The objective of monotonicity is to prove that $a_{n+1}>a_{n}$ for any $n\geq 1$ by induction.

   \begin{rlist}
     \item When $n=1$, $a_{1}=1$ and $a_{2}=\sqrt{2}$. Hence $a_{2}>a_{1}$.

     \item Assume that $a_{k+1}>a_{k}$ for some $k\in\Z^{+}$. Then

     $$a_{k+2}=\sqrt{a_{k+1}+1}>\sqrt{a_{k}+1}=a_{k+1}$$
   \end{rlist}

   Then $a_{n}$ is strictly increasing.

   \item \textbf{Using the Monotone Convergence Theorem}\n

   The infinite sequence is proved to be bounded and strictly increasing. Therefore, by the Monotone Convergence Theorem, the infinite sequence is convergent and its limit exists.

   \item \textbf{Finding the Limit}\n

   Suppose $\lim_{n\to\infty}a_{n}=a$. Then $\lim_{n\to\infty}a_{n+1}=a$ and thus

   $$\begin{aligned}[t]
     a&=\sqrt{a+1}\\
     a^{2}&=a+1\\
     a^{2}-a-1&=0
   \end{aligned}$$\s

   By solving the quadratic equation,

   $$a=\frac{1+\sqrt{5}}{2}\erm{or }a=\frac{1-\sqrt{5}}{2}$$\s

   Since the infinite sequence is always greater than $1$, then the former one is the solution.
   \end{alist}
\end{exm}

\subsubsection{Squeeze Theorem}
Here \textbf{Squeeze Theorem} (or \textbf{Sandwich Theorem}) is introduced:\n

\begin{thm}
  Suppose $a_{n}$, $b_{n}$ and $c_{n}$ are infinite sequences such that the inequality $a_{n}\leq b_{n}\leq c_{n}$ for any $n\in\Z^{+}$ holds, and $\lim_{n\to\infty}a_{n}=\lim_{n\to\infty}c_{n}=L$, then $b_{n}$ is convergent and $\lim_{n\to\infty}b_{n}=L$.
\end{thm}

\subsection{Limits of Functions}
\subsubsection{Functions}
\begin{dft}
  A real-valued \textbf{function} on a subset $D\subset \R$ is a real value $f(x)$ assigned to each of the values $x\in D$. The set $D$ is called the \textbf{domain} of the function.\n

  If the function, as defined as above, with its domain $D$ understood to be taken as the set of all real numbers $x$ such that $f(x)$ is defined. Then, $D$ can also be called the \textbf{maximum domain} of definition of $f(x)$.
\end{dft}

\subsubsection{Injectivity and Surjectivity}
\begin{dft}
  Denote a real-valued function as $f(x)$ with domain $D$. Then the injectivity and surjectivity of the function can be determined as below:
  \begin{alist}
    \item The real-valued function is said to be \textbf{injective} if for any $x_{1}\text{,}x_{2}\in D$ with $x_{1}\neq x_{2}$, then $f(x_{1})\neq f(x_{2})$.

    \item The real-valued function is said to be \textbf{surjective} if for any real number $y\in\R$, there exists $x\in D$ such that $f(x)=y$.

    \item The real-valued function is said to be \textbf{bijective} if the function is injective and surjective at the same time.
  \end{alist}
\end{dft}

\subsubsection{Evenness and Oddness}
\begin{dft}
  Denote a real-valued function as $f(x)$. Then the evenness and oddness of the function can be determined as below:
  \begin{alist}
    \item The real-valued function is said to be \textbf{even} if the following equation applies to the function:

    $$f(-x)=f(x)\erm{for any }x$$

    \item The real-valued function is said to be \textbf{odd} if the following equation applies to the function:

    $$f(-x)=-f(x)\erm{for any }x$$
  \end{alist}
\end{dft}

\subsubsection{Existence of Limits of Functions}
\begin{thm}
  Denote a real-valued function as $f(x)$.
  \begin{alist}
    \item Denote a real number as $l$. Limit of the real-valued function at $x=a$ is $l$ if for any $\epsilon>0$, there exists $\delta>0$ such that the following applies:

    $$\text{If }0<\abs{x-a}<\delta\text{, then }\abs{f(x)-l}<\epsilon$$\s

    Then the limit $\lim_{x\to a}f(x)=l$.

    \item Denote a real number as $l$. Limit of the real-valued function when $x$ tends to $+\infty$ if for any $\epsilon>0$, there exists $R>0$ such that the following applies:

    $$\text{If }x>R\text{, then }\abs{f(x)-l}<\epsilon$$\s

    Then the limit $\lim_{x\to+\infty}f(x)=l$.
  \end{alist}
\end{thm}\n

Limit of the real-valued function when $x$ tends to $-\infty$ is defined similarly.\n

\begin{thm}
  Denote a real-valued function as $f(x)$. The Sequential Criterion for Limits of Functions can be applied as below:

  $$\lim_{x\to a}f(x)=l$$\s

  if and only if for any sequence of real numbers with $\lim_{n\to\infty}x_{n}=a$, the following will be true:

  $$\lim_{n\to\infty}f(x_{n})=l$$
\end{thm}

\subsubsection{Manipulations of Limits of Functions}
\begin{pst}
  Denote real-valued functions as $f(x)$ and $g(x)$ such that $\lim_{x\to a}f(x)$ and $\lim_{x\to a}g(x)$ exist. Also denote a real number as $k$. Then the following manipulations of limits can be applied:

  \begin{alist}
    \item \textbf{Addition of Limits}

    $$\lim_{x\to a}(f(x)+g(x))=\lim_{x\to a}f(x)+\lim_{x\to a}g(x)$$

    \item \textbf{Scalar Multiplication of Limits}

    $$\lim_{x\to a}kf(x)=k\lim_{x\to a}f(x)$$

    \item \textbf{Multiplication of Limits}

    $$\lim_{x\to a}f(x)g(x)=\lim_{x\to a}f(x)\lim_{x\to a}g(x)$$

    \item \textbf{Division of Limits}\n

    Suppose $\lim_{x\to a}g(x)=0$. Then

    $$\lim_{x\to a}\frac{f(x)}{g(x)}=\frac{\lim_{x\to a}f(x)}{\lim_{x\to a}g(x)}$$
  \end{alist}
\end{pst}

\subsection{Exponential, Logarithmic and Trigonometric Functions}
\subsubsection{Exponential Function}
\begin{dft}
  The \textbf{exponential function} is defined for real number $x\in \R$ by the following:

  $$\begin{aligned}[t]
    e^{x}&=\lim_{n\to\infty}\brr{1+\frac{x}{n}}^{n}\\
    &=1+x+\frac{x^{2}}{2!}+\frac{x^{3}}{3!}+\frac{x^{4}}{4!}+\cdots
  \end{aligned}$$
\end{dft}\n

The two limits in the definition above exists and converge to the same value for any real number $x$. Also, note that $e^{x}$ (or $exp(x)$) is a notation for the exponential, and it should not be interpreted as '$e$ to the power $x$'.\n

Note that the exponential function has the following property:

$$e^{x+y}=e^{x}e^{y}$$\s

As it is said above, $e^{x}$ does not simply represent '$e$ to the power $x$', hence this cannot be interpreted as using law of indices.\n

The exponential function has property of boundedness:\n

\begin{pst}
  $e^{x}>0$ for any real number $x$.\n

  \prf For any $x>0$, $e^{x}>1+x>1$. If $x<0$, then

  $$\begin{aligned}[t]
    e^{x}e^{-x}&=e^{x+(-x)}=e^{0}=1\\
    e^{x}&=\frac{1}{e^{-x}}>0
  \end{aligned}$$\s

  since $e^{-x}>1$. Therefore $e^{x}>0$ for any $x\in \R$.
\end{pst}\n

Besides the boundedness, the exponential function also has the property of monotonicity.\n

\begin{pst}
  $e^{x}$ is strictly increasing.\n

  \prf Let $x$, $y$ be real numbers with $x<y$. Then $y-x>0$ which implies $e^{y-x}>1$. Therefore

  $$e^{y}=e^{x+(y-x)}=e^{x}e^{y-x}>e^{x}$$
\end{pst}

\subsubsection{Logarithmic Function}
\begin{dft}
  The \textbf{logarithmic function} is the function $\ln:\R^{+}\to\R$ defined to for $x>0$ by

  $$y=\ln(x)\erm{if }e^{y}=x$$
\end{dft}\n

In other words, $\ln(x)$ is the inverse function of $e^{x}$.\n

\begin{pst}
  The logarithmic function has the following properties of manipulation:

  \begin{alist}
    \item \textbf{Addition of Logarithmic Functions}

    $$\ln(xy)=\ln(x)+\ln(y)$$

    \item \textbf{Subtraction of Logarithmic Functions}

    $$\ln\brr{\frac{x}{y}}=\ln(x)-\ln(y)$$

    \item \textbf{Powers of Logarithmic Functions}

    $$\ln(x^{n})=n\ln(x)\erm{for any }n\in \Z$$
  \end{alist}

  \prf Let $u=\ln(x)$ and $v=\ln(y)$. Then $e^{u}=x$ and $e^{v}=y$ by definition, and

  $$\begin{aligned}[t]
    xy&=e^{u}e^{v}=e^{u+v}=e^{\ln(x)+\ln(y)}\\
    \ln(xy)&=\ln(x)+\ln(y)
  \end{aligned}$$\s

  This proves the first part of the list above, and other parts can also be proved similarly.
\end{pst}

\subsubsection{Trigonometric Functions}
\begin{dft}
  The \textbf{sine function} is defined for real number $x\in \R$ by the infinite series

  $$\sin(x)=x-\frac{x^{3}}{3!}+\frac{x^{5}}{5!}-\frac{x^{7}}{7!}+\cdots$$\s

  Similarly, the \textbf{cosine function} is defined for real number $x\in \R$ by the infinite series

  $$\cos(x)=1-\frac{x^{2}}{2!}+\frac{x^{4}}{4!}-\frac{x^{6}}{6!}+\cdots$$
\end{dft}\n

Note that when the sine and cosine are interpreted as trigonometric ratios, the angles are not measured in degrees but in radians. Also, both infinite series for cosine function and sine function are convergent for any real number $x\in \R$.\n

\begin{dft}
  With the definition of sine function and cosine function, another four trigonometric functions, including the \textbf{tangent function}, \textbf{secant function}, \textbf{cosecant function} and \textbf{cotangent function}, can be defined with sine function and cosine function:

  $$\tan(x)=\frac{\sin(x)}{\cos(x)}\erm{for }x\neq\frac{2k+1}{2}\pi\erm{where }k\in\Z$$\s

  $$\sec(x)=\frac{1}{\cos(x)}\erm{for }x\neq\frac{2k+1}{2}\pi\erm{where }k\in\Z$$\s

  $$\csc(x)=\frac{1}{\sin(x)}\erm{for }x\neq k\pi\erm{where }k\in\Z$$\s

  $$\cot(x)=\frac{\cos(x)}{\sin(x)}\erm{for }x\neq k\pi\erm{where }k\in\Z$$
\end{dft}

\subsubsection{Trigonometric Identities}
\begin{pst}
  The following is a list of trigonometric identities:

  \begin{alist}
    \item \textbf{Pythagorean Identities}

    \begin{rlist}
      \item Sine Function and Cosine Function

      $$\sin^{2}(x)+\cos^{2}(x)=1$$

      \item Tangent Function and Secant Function

      $$1+\tan^{2}(x)=\sec^{2}(x)$$

      \item Cotangent Function and Cosecant Function

      $$1+\cot^{2}(x)=\csc^{2}(x)$$
    \end{rlist}

    \item \textbf{Angle Sum and Difference Identities}

    \begin{rlist}
      \item Sine Function

      $$\sin(\alpha\pm\beta)=\sin(\alpha)\cos(\beta)\pm\cos(\alpha)\sin(\beta)$$

      \item Cosine Function

      $$\sin(\alpha\pm\beta)=\cos(\alpha)\cos(\beta)\mp\sin(\alpha)\sin(\beta)$$

      \item Tangent Function

      $$\tan(\alpha\pm\beta)=\frac{\tan(\alpha)\pm\tan(\beta)}{1\mp \tan(\alpha)\tan(\beta)}$$
    \end{rlist}

    \item \textbf{Double-angle Formulae}

    \begin{rlist}
      \item Sine Function

      $$\sin(2x)=2\sin(x)\cos(x)$$

      \item Cosine Function

      $$\cos(2x)=\cos^{2}(x)-\sin^{2}(x)$$

      \item Tangent Function

      $$\tan(2x)=\frac{2\tan(x)}{1-\tan^{2}(x)}$$
    \end{rlist}

    \item \textbf{Product-to-sum Identities}

    \begin{rlist}
      \item Product of Two Sine Functions

      $$2\sin(\alpha)\sin(\beta)=-\cos(\alpha+\beta)+\cos(\alpha-\beta)$$

      \item Product of Two Cosine Functions

      $$2\cos(\alpha)\cos(\beta)=\cos(\alpha+\beta)+\cos(\alpha-\beta)$$

      \item Product of Sine and Cosine Functions

      $$2\sin(\alpha)\cos(\beta)=\sin(\alpha+\beta)-\sin(\alpha-\beta)$$
    \end{rlist}

    \item \textbf{Sum-to-product Identities}

    \begin{rlist}
      \item Sum of Two Sine Functions

      $$\sin(\alpha)+\sin(\beta)=2\sin\brr{\frac{\alpha+\beta}{2}}\cos\brr{\frac{\alpha-\beta}{2}}$$

      \item Difference of Two Sine Functions

      $$\sin(\alpha)-\sin(\beta)=2\cos\brr{\frac{\alpha+\beta}{2}}\sin\brr{\frac{\alpha-\beta}{2}}$$

      \item Sum of Two Cosine Functions

      $$\cos(\alpha)+\cos(\beta)=2\cos\brr{\frac{\alpha+\beta}{2}}\cos\brr{\frac{\alpha-\beta}{2}}$$

      \item Difference of Two Cosine Functions

      $$\cos(\alpha)-\cos(\beta)=-2\sin\brr{\frac{\alpha+\beta}{2}}\sin\brr{\frac{\alpha-\beta}{2}}$$
    \end{rlist}
  \end{alist}
\end{pst}

\subsubsection{Hyperbolic Functions and its Identities}
\begin{dft}
  The \textbf{hyperbolic sine function} is defined for real number $x\in\R$ by

  $$\sinh(x)=\frac{e^{x}-e^{-x}}{2}=x+\frac{x^{3}}{3!}+\frac{x^{5}}{5!}+\frac{x^{7}}{7!}+\cdots$$

  Similarly, the \textbf{hyperbolic cosine function} is defined for real number $x\in\R$ by

  $$\cosh(x)=\frac{e^{x}+e^{-x}}{2}=1+\frac{x^{2}}{2!}+\frac{x^{4}}{4!}+\frac{x^{6}}{6!}+\cdots$$
\end{dft}\n

With the definition of hyperbolic functions, the following identities, although not often to be used, can be applied:\n

\begin{pst}
  Below are the identities for hyperbolic functions:

  \begin{alist}
    \item \textbf{Difference of Hyperbolic Cosine Function and Hyperbolic Sine Function}

    $$cosh^{2}(x)-sinh^{2}(x)=1$$

    \item \textbf{Angle Sum and Difference of Hyperbolic Sine Function}

    $$\sinh(\alpha\pm\beta)=\sinh(\alpha)\cosh(\beta)\pm\cosh(\alpha)\sinh(\beta)$$

    \item \textbf{Angle Sum and Differencce of Hyperbolic Cosine Function}

    $$\cosh(\alpha\pm\beta)=\cosh(\alpha)\cosh(\beta)\mp\sinh(\alpha)\sinh(\beta)$$

    \item \textbf{Double-angle Formula of Hyperbolic Sine Function}

    $$\sinh(2x)=2\sinh(x)\cosh(x)$$

    \item \textbf{Double-angle Formula of Hyperbolic Cosine Function}

    $$\cosh(2x)=\cosh^{2}(x)-\sinh^{2}(x)$$
  \end{alist}
\end{pst}\n

Note that some identities for hyperbolic functions are quite similar to those for standard trigonometric functions.

\subsubsection{Common Limits of Functions that Tends to Zero}
\begin{pst}
  Here are some common limits of functions that tends to zero and is always used for finding the limit:

  \begin{alist}
    \item \textbf{Limit of Exponential Function versus $\boldsymbol{x}$}

    $$\lim_{x\to 0}\frac{e^{x}-1}{x}=1$$

    \item \textbf{Limit of Logarithmic Function versus $\boldsymbol{x}$}

    $$\lim_{x\to 0}\frac{\ln(x+1)}{x}=1$$

    \item \textbf{Limit of Sine Function versus $\boldsymbol{x}$}

    $$\lim_{x\to 0}\frac{\sin(x)}{x}=1$$
  \end{alist}

  \prf\prt[a]{zr} For any $-1<x<1$ with $x\neq 0$,

  $$\begin{aligned}[t]
    \frac{e^{x}-1}{x}&=1+\frac{x}{2!}+\frac{x^{2}}{3!}+\frac{x^{3}}{4!}+\frac{x^{4}}{5!}+\cdots\\
    &\leq 1+\frac{x}{2}+\brr{\frac{x^{2}}{4}+\frac{x^{3}}{8}+\frac{x^{4}}{16}+\cdots}=1+\frac{x}{2}+\frac{x^{2}}{2}
  \end{aligned}$$\s

  At the same time,

  $$\begin{aligned}[t]
    \frac{e^{x}-1}{x}&=1+\frac{x}{2!}+\frac{x^{2}}{3!}+\frac{x^{3}}{4!}+\cdots\\
    &\geq 1+\frac{x}{2}-\brr{\frac{x^{2}}{4}+\frac{x^{3}}{8}+\frac{x^{4}}{16}+\cdots}=1+\frac{x}{2}-\frac{x^{2}}{2}
  \end{aligned}$$\s

  Note that

  $$\lim_{x\to 0}\brr{1+\frac{x}{2}+\frac{x^{2}}{2}}=\lim_{x\to 0}\brr{1+\frac{x}{2}-\frac{x^{2}}{2}}=1$$\s

  Therefore

  $$\lim_{x\to 0}\frac{e^{x}-1}{x}=1$$\s

  \prtc[b]{zr} Let $y=\ln(1+x)$, then

  $$\begin{aligned}[t]
    e^{y}&=1+x\\
    x&=e^{y}-1
  \end{aligned}$$\s

  and $x\to 0$ as $y\to 0$. Therefore

  $$\lim_{x\to 0}\frac{\ln(1+x)}{x}=\lim_{y\to 0}\frac{y}{e^{y}-1}=1$$\s

  \prtc[c]{zr} Note that

  $$\frac{\sin(x)}{x}=1-\frac{x^{2}}{3}+\frac{x^{4}}{5}-\frac{x^{6}}{7}+\frac{x^{8}}{9}-\frac{x^{10}}{11}+\cdots$$\s

  For any $-1<x<1$ with $x\neq 0$, we have

  $$\begin{aligned}[t]
    \frac{sin(x)}{x}&=1-\brr{\frac{x^{2}}{3!}-\frac{x^{4}}{5!}}-\brr{\frac{x^{6}}{7!}-\frac{x^{8}}{9!}}-\cdots \leq 1\\
    \frac{sin(x)}{x}&=1-\frac{x^{2}}{3!}+\brr{\frac{x^{4}}{5!}-\frac{x^{6}}{7!}}+\brr{\frac{x^{8}}{9!}-\frac{x^{10}}{11!}}+\cdots \geq 1-\frac{x^{2}}{6}
  \end{aligned}$$\s

  and

  $$\lim_{x\to 0}1=\lim_{x\to 0}\brr{1-\frac{x^{2}}{6}}=1$$

  Therefore

  $$\lim_{x\to 0}\frac{\sin(x)}{x}=1$$
\end{pst}

\subsubsection{Common Limits of Functions that Tends to Infinity}
\begin{pst}
  Here are some common limits of functions that tends to infinity and is always used for finding the limit:

  \begin{alist}
    \item \textbf{Limit of Monomial versus Exponential Function}\\
    Let $k$ be a positive integer, then

    $$\lim_{x\to+\infty}\frac{x^{k}}{e^{x}}=0$$

    \item \textbf{Limit of Logarithmic Function to Certain Power versus $\boldsymbol{x}$}\\
    Let $k$ be a positive integer, then

    $$\lim_{x\to+\infty}\frac{(\ln(x))^{k}}{x}=0$$
  \end{alist}

  \prf\prt[a]{zr} For any $x>0$,

  $$e^{x}=1+x+\frac{x^{2}}{2!}+\frac{x^{3}}{3!}+\cdots>\frac{x^{x+1}}{(k+1)!}$$\s

  and thus

  $$0<\frac{x^{k}}{e^{x}}<\frac{(k+1)!}{x}$$\s

  Note that

  $$\lim_{x\to+\infty}\frac{(k+1)!}{x}=0$$\s

  Therefore

  $$\lim_{x\to+\infty}\frac{x^{k}}{e^{x}}=0$$\s

  \prtc[b]{zr} Let $x=e^{y}$, then $\ln(x)=y$. Note that $x\to+\infty$ as $y\to+\infty$. Therefore

  $$\lim_{x\to+\infty}\frac{(\ln(x))^{k}}{x}=\lim_{y\to+\infty}\frac{y^{k}}{e^{y}}=0$$
\end{pst}

\subsection{Continuity of Functions}
\subsubsection{Continuity at Points and Intervals}
\begin{dft}
  Let $f(x)$ be a real-valued function. $f(x)$ is said to be \textbf{continuous} at $x=a$ if

  $$\lim_{x\to a}f(x)=f(a)$$\s

  In other words, $f(x)$ is continuous at $x=a$ if for any $\epsilon>0$, there exists $\delta>0$ such that the following applies:

  $$\text{If }0<\abs{x-a}<\delta\text{, then }\abs{f(x)-f(a)}<\epsilon$$
\end{dft}\n

$f(x)$ is continuous on an interval in $\R$ if $f(x)$ is continuous at every point on the interval.\n

\begin{pst}
  The following types of function are continuous on $\R$:

  \begin{alist}
    \item \textbf{Monomial with Non-negative Degrees}\n

    For any non-negative integer $n$, $f(x)=x^{n}$ is continuous on $\R$.

    \item \textbf{Sine and Cosine Functions}\n

    $\sin(x)$ and $\cos(x)$ are continuous on $\R$.

    \item \textbf{Exponential Function}\n

    $e^{x}$ is continuous on $\R$.

    \item \textbf{Logarithmic Function}\n

    $\ln(x)$ is continuous on $\R^{+}$.
  \end{alist}
\end{pst}

\subsubsection{Continuity of Combinations of Functions}
\begin{thm}
  Let $g(u)$ be a real-valued function in $u$, and $f(x)$ be a real-valued function in $x$ with relation $u=f(x)$.\n

  Suppose $g(u)$ is continuous and the limit of $f(x)$ at $x=a$ exists, then

  $$\lim_{x\to a}(g\circ f)(x)=\lim_{x\to a}g(f(x))=g(\lim_{x\to a}f(x))$$
\end{thm}\n

\begin{pst}
  Suppose $f(x)$ and $g(x)$ are continuous functions and $k$ is a real number, then the following functions are all continuous:

  \begin{alist}
    \item \textbf{Addition of Functions}

    $$f(x)+g(x)\erm{is continuous.}$$

    \item \textbf{Scalar Multiplication of Functions}

    $$kf(x)\erm{is continuous.}$$

    \item \textbf{Multiplication of Functions}

    $$f(x)g(x)\erm{is continuous.}$$

    \item \textbf{Division of Functions}\\
    Suppose $g(x)\neq 0$ at all points within the interval, then

    $$\frac{f(x)}{g(x)}\erm{is continuous.}$$

    \item \textbf{Nested Functions}

    $$(f\circ g)(x)\erm{is continuous.}$$
  \end{alist}
\end{pst}

\subsubsection{Piecewise Defined Functions}
\begin{dft}
  A \textbf{piecewise defined function} is a function defined by multiple sub-functions, with each of the sub-functions applying to a certain interval of the domain of the main function, called as a sub-domain.
\end{dft}

\subsubsection{Absolute Values}
\begin{dft}
  The \textbf{absolute value} of $x\in \R$ is defined by the following:

  $$\abs{x}=\begin{cases}
    -x&\erm{if }x<0\\
    x&\erm{if }x\geq 0
  \end{cases}$$

  Another way to define absolute value of $x\in \R$ without involving piecewise defined function is:

  $$\abs{x}=\sqrt{x^{2}}$$
\end{dft}

\subsubsection{One-sided Limits}
Because of the nature of piecewise defined functions, left-hand limits and right-hand limits are useful tools to check if the function is continuous or not.\n

\begin{thm}
  Denote a real-valued function as $f(x)$. The function is continuous at $x=a$ if

  $$\lim_{x\to a^{-}}f(x)=\lim_{x\to a^{+}}f(x)=f(a)$$
\end{thm}

\subsection{Theorems on Continuity of Functions}
\subsubsection{Intermediate Value Theorem}
With the continuity of functions, \textbf{Intermediate Value Theorem} is introduced:\n

\begin{thm}
  Denote a real-valued function as $f(x)$. Suppose the function is continuous on interval $[a,b]$. Then for any real number $\eta$ between $f(a)$ and $f(b)$, there exists some $\zeta\in (a,b)$ such that $f(\zeta)=\eta$.
\end{thm}

\subsubsection{Extreme Value Theorem}
Besides the Intermediate Value Theorem, \textbf{Extreme Value Theorem} also uses the continuity of functions:\n

\begin{thm}
  Denote a real-valued function as $f(x)$. Suppose the function is continuous on a closed and bounded interval $[a,b]$. Then there exists some $\alpha,\beta\in[a,b]$ such that the following is always true:

  $$f(\alpha)\leq f(x)\leq f(\beta)\erm{for any }x\in[a,b]$$
\end{thm}

\pagebreak

\section{Differentiation}
\subsection{Differentiable Functions}
\subsubsection{Differentiability of Functions}
\begin{dft}
  Denote a real-valued function as $f(x)$. Denote the limit for differentiability of the function by the following:

  $$f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}$$\s

  The function is said to be \textbf{differentiable} at $x=a$ if the above limit exists. The function is also said to be differentiable on $(a,b)$ if $f(x)$ is differentiable at every point in $(a,b)$.\n

  The another form of limit for differentiability of the function is the following:

  $$f'(a)=\lim_{x\to a}\frac{f(x)-f(a)}{x-a}$$
\end{dft}

\subsubsection{Relation from Differentiability to Continuity}
With the definition of differentiability, the relation between differentiability and continuity of functions can be introduced:\n

\begin{thm}
  Denote a real-valued function as $f(x)$. If $f(x)$ is differentiable at $x=a$, then $f(x)$ is continuous at $x=a$.\n

  \prf Denote a real-valued function as $f(x)$. Suppose $f(x)$ is differentiable at $x=a$. Then

  $$\begin{aligned}[t]
    \lim_{x\to a}(f(x)-f(a))&=\lim_{x\to a}\brr{\frac{f(x)-f(a)}{x-a}\cdot (x-a)}\\
    &=\lim_{x\to a}\frac{f(x)-f(a)}{x-a}\lim_{x\to a}(x-a)\\
    &=f'(a)\cdot 0=0
  \end{aligned}$$\s

  Therefore $f(x)$ is continuous at $x=a$.
\end{thm}\n

Although the continuity of the function is guaranteed with the differentiability of the function, the converse of the above theorem does not hold.

\subsubsection{First Derivatives}
\begin{dft}
  Denote a real-valued function as $f(x)$. Suppose $f(x)$ is differentiable on interval $(a,b)$. The \textbf{first derivative} of $f(x)$ is the function on $(a,b)$ defined by the following:

  $$\frac{\diff y}{\diff x}=f'(x)=\lim_{x\to 0}\frac{f(x+h)-f(x)}{h}$$
\end{dft}

\subsection{Rules of Differentiation}
\subsubsection{Basic Formulae of Differentiation}
\begin{pst}
  Here are some basic formulae of differentiation:

  \begin{alist}
    \item \textbf{Differentiation of Monomials (Power Rule)}

    $$\frac{\diff}{\diff x}x^{n}=nx^{n-1}\erm{for }n\in\R\setminus\brc{0}$$

    \item \textbf{Differentiation of Exponential Function}

    $$\frac{\diff}{\diff x}e^{x}=e^{x}$$

    \item \textbf{Differentiation of Logarithmic Function}

    $$\frac{\diff}{\diff x}\ln(x)=\frac{1}{x}$$

    \item \textbf{Differentiation of Sine Function}

    $$\frac{\diff}{\diff x}\sin(x)=\cos(x)$$

    \item \textbf{Differentiation of Cosine Function}

    $$\frac{\diff}{\diff x}\cos(x)=-\sin(x)$$

    \item \textbf{Differentiation of Tangent Function}

    $$\frac{\diff}{\diff x}\tan(x)=\sec^{2}(x)$$

    \item \textbf{Differentiation of Secant Function}

    $$\frac{\diff}{\diff x}\sec(x)=\sec(x)\tan(x)$$

    \item \textbf{Differentiation of Cosecant Function}

    $$\frac{\diff}{\diff x}\csc(x)=-\csc(x)\cot(x)$$

    \item \textbf{Differentiation of Cotangent Function}

    $$\frac{\diff}{\diff x}\cot(x)=-\csc^{2}(x)$$

    \item \textbf{Differentiation of Hyperbolic Sine Function}

    $$\frac{\diff}{\diff x}\sinh(x)=\cosh(x)$$

    \item \textbf{Differentiation of Hyperbolic Cosine Function}

    $$\frac{\diff}{\diff x}\cosh(x)=\sinh(x)$$
  \end{alist}

  \prf\prt[a]{zr} Let $y=x^{n}$. Suppose $n$ is a positive integer. For any $x\in \R$,

  $$\begin{aligned}[t]
    \frac{\diff y}{\diff x}&=\lim_{h\to 0}\frac{(x+h)^{n}-x^{n}}{h}\\
    &=\lim_{h\to 0}\frac{(x+h-x)((x+h)^{n-1}+(x+h)^{n-2}x+\cdots+x^{n-1})}{h}\\
    &=\lim_{h\to 0}(x+h-x)(x+h)^{n-1}+(x+h)^{n-2}x+\cdots+x^{n-1}=nx^{n-1}
  \end{aligned}$$\s

  In fact, the power rule can be applied for any real number $n$ except $0$. The complete proof (negative and rational indices to be exact) can only be done after product rule and chain rule is introduced.\n

  \prtc[b]{zr} Let $y=e^{x}$. For any $x \in \R$,

  $$\frac{\diff y}{\diff x}=\lim_{h\to 0}\frac{e^{x+h}-e^{x}}{h}=\lim_{h\to 0}\frac{e^{x}(e^{h}-1)}{h}=e^{x}$$

  \prtc[c]{zr} Let $y=\ln(x)$. For any $x>0$,

  $$\frac{\diff y}{\diff x}=\lim_{h\to 0}\frac{\ln(x+h)-\ln(x)}{h}=\lim_{h\to 0}\frac{\ln(1+\frac{h}{x})}{h}=\frac{1}{x}$$

  \prtc[d]{zr} Let $y=sin(x)$. For any $x\in \R$,

  $$\frac{\diff y}{\diff x}=\lim_{h\to 0}\frac{sin(x+h)-sin(x)}{h}=\lim_{h\to 0}\frac{2cos(x+\frac{h}{2})sin(\frac{h}{2})}{h}=cos(x)$$

  \prtc[e]{zr} Let $y=cos(x)$. For any $x\in \R$,

  $$\frac{\diff y}{\diff x}=\lim_{h\to 0}\frac{cos(x+h)-cos(x)}{h}=\lim_{h\to 0}\frac{-2sin(x+\frac{h}{2})sin(\frac{h}{2})}{h}=-sin(x)$$
\end{pst}\n

\begin{pst}
  Denote differentiable functions as $f(x)$ and $g(x)$. Also denote a real number as $k$. Then the following equations can be applied:

  \begin{alist}
    \item \textbf{Addition of First Derivatives}

    $$(f+g)'(x)=f'(x)+g'(x)$$

    \item \textbf{Scalar Multiplication of First Derivatives}

    $$(kf)'(x)=kf'(x)$$
  \end{alist}
\end{pst}

\subsubsection{Exponential Functions with Positive Integer Bases}
\begin{dft}
  Let $a$ be a positive real number. For $x\in\R$, the \textbf{exponential function} with base $a$ is defined as

  $$a^{x}=e^{x\ln(a)}$$
\end{dft}\n

\begin{pst}
  Exponential functions with integer bases have the following properties:

  \begin{alist}
    \item \textbf{Addition of Indices in Exponential Functions with Positive Integer Bases}

    $$a^{x+y}=a^{x}a^{y}$$

    \item \textbf{Derivative of Exponential Functions with Positive Integer Bases}

    $$\frac{\diff}{\diff x}a^{x}=a^{x}\ln(a)$$
  \end{alist}

  \prf\prt[a]{zr} For any $a>0$,

  $$a^{x+y}=e^{(x+y)\ln(a)}=e^{x\ln(a)}e^{y\ln(a)}=a^{x}a^{y}$$\s

  \prtc[b]{zr} For any $a>0$,

  $$\frac{\diff}{\diff x}a^{x}=\frac{\diff}{\diff x}e^{x\ln(a)}=e^{x\ln(a)}\ln(a)=a^{x}\ln(a)$$
\end{pst}

\subsubsection{Product Rule and Quotient Rule}
Here \textbf{product rule} and \textbf{quotient rule} is introduced:\n

\begin{thm}
  Denote differentiable functions of $x$ as $p$ and $q$, then the following can be applied:

  \begin{alist}
    \item \textbf{Product Rule}

    $$\frac{\diff}{\diff x}(pq)=p\frac{\diff q}{\diff x}+q\frac{\diff p}{\diff x}$$

    \item \textbf{Quotient Rule}

    $$\frac{\diff}{\diff x}\brr{\frac{p}{q}}=\frac{q\frac{\diff p}{\diff x}-p\frac{\diff q}{\diff x}}{q^{2}}$$
  \end{alist}

  \prf\prt[a]{zr} Denote differentiable functions of $x$ as $p=f(x)$ and $q=g(x)$. Then

  $$\begin{aligned}[t]
    \frac{\diff}{\diff x}(pq)&=\lim_{h\to 0}\frac{f(x+h)g(x+h)-f(x)g(x)}{h}\\
    &=\lim_{h\to 0}\brr{\frac{f(x+h)g(x+h)-f(x+h)g(x)}{h}+\frac{f(x+h)g(x)-f(x)g(x)}{h}}\\
    &=\lim_{h\to 0}\brr{f(x+h)\brr{\frac{g(x+h)-g(x)}{h}}-g(x)\brr{\frac{f(x+h)-f(x)}{h}}}\\
    &=p\frac{\diff q}{\diff x}+q\frac{\diff p}{\diff x}
  \end{aligned}$$\s

  \prtc[b]{zr} Denote differentiable functions of $x$ as $p=f(x)$ and $q=g(x)$. Then

  $$\begin{aligned}[t]
    \frac{\diff}{\diff x}\brr{\frac{p}{q}}&=\lim_{h\to 0}\frac{\frac{f(x+h)}{g(x+h)}-\frac{f(x)}{g(x)}}{h}\\
    &=\lim_{h\to 0}\frac{f(x+h)g(x)-f(x)g(x+h)}{hg(x)g(x+h)}\\
    &=\lim_{h\to 0}\brr{\frac{f(x+h)g(x)-f(x)g(x)}{hg(x)g(x+h)}-\frac{f(x)g(x+h)-f(x)g(x)}{hg(x)g(x+h)}}\\
    &=\lim_{h\to 0}\brr{g(x)\brr{\frac{f(x+h)-f(x)}{hg(x)g(x+h)}}-f(x)\brr{\frac{g(x+h)-g(x)}{hg(x)g(x+h)}}}\\
    &=\frac{q\frac{\diff p}{\diff x}-p\frac{\diff q}{\diff x}}{q^{2}}
  \end{aligned}$$
\end{thm}

\subsubsection{Chain Rule}
Besides product rule and quotient rule, \textbf{chain rule} is also useful for finding the derivative:\n

\begin{thm}
  Denote a function of $u$ as $y=f(u)$ and a function of $x$ as $u=g(x)$. Suppose $g(x)$ is differentiable at $x=a$ and $f(u)$ is also differentiable at $u=g(a)$, then $(f\circ g)(x)=f(g(x))$ is differentiable at $x=a$ and

  $$(f\circ g)'(a)=f'(g(a))g'(a)$$\s

  \prf Denote a function of $u$ as $y=f(u)$ and a function of $x$ as $u=g(x)$. Suppose $g(x)$ is differentiable at $x=a$ and $f(u)$ is also differentiable at $u=g(a)$. Then

  $$\begin{aligned}[t]
    \frac{\diff}{\diff x}(f\circ g'(a))&=\lim_{h\to 0}\frac{f(g(a+h))-f(g(a))}{h}\\
    &=\lim_{h\to 0}\frac{f(g(a+h))-f(g(a))}{g(a+h)-g(a)}\lim_{h\to 0}\frac{g(a+h)-g(a)}{h}
  \end{aligned}$$\s

  Further let $k=g(a+h)-g(a)$. Note that $g(a+h)-g(a)=k\to 0$ as $h\to 0$ since $g(x)$ is continuous. Then

  $$\begin{aligned}[t]
    \frac{\diff}{\diff x}(f\circ g'(a))&=\lim_{h\to 0}\frac{f(g(a+h))-f(g(a))}{g(a+h)-g(a)}\lim_{h\to 0}\frac{g(a+h)-g(a)}{h}\\
    &=\lim_{k\to 0}\frac{f(g(a)+k)-f(g(a))}{k}\lim_{h\to 0}\frac{g(a+h)-g(a)}{h}\\
    &=f'(g(a))g'(a)
  \end{aligned}$$
\end{thm}

However, the proof above is only valid if $g(a+h)-g(a)\neq 0$ whenever $h$ is sufficiently close to $0$. The following proposition is introduced to show that the above statement is true when $g'(a)\neq 0$:\n

\begin{pst}
  Suppose $g(x)$ is a function such that $g'(a)\neq 0$. There exists some $\delta>0$ such that if $0<\abs{h}<\delta$, then $g(a+h)-g(a)\neq 0$.
\end{pst}\n

Another proposition and proof is required for $g'(a)=0$ in order to complete the general proof.\n

\begin{pst}
  Suppose $f(u)$ is a function which is differentiable at $u=b$. There exists some $\delta>0$ and $M>0$ such that

  $$\abs{f(b+h)-f(b)}<M\abs{h}\erm{for any }\abs{h}<\delta$$
\end{pst}\n

With the above proposition, it can be stated that there exists some $\delta>0$ such that

$$\abs{f(g(a+h))-f(g(a))}<M\abs{g(a+h)-g(a)}\erm{for any }\abs{h}<\delta$$\s

By applying the squeeze theorem,

$$\lim_{h\to 0}\abs{\frac{f(g(a+h))-f(g(a))}{h}}\leq\lim_{h\to 0}M\abs{\frac{g(a+h)-g(a)}{h}}=0$$\s

which implies $(f\circ g)'(a)=0$.

\subsection{Introduction to Implicit Functions}
\subsubsection{Implicit Functions}
\begin{dft}
  An \textbf{implicit function} is an equation of the form $F(x,y)=0$. Note that an implicit function is not necessary to be well-defined, depending on its domain and range.
\end{dft}

\subsubsection{Implicit Differentiation}
\begin{dft}
  Denote an implicit function as $F(x,y)=0$. By differentiating both sides with respect to $x$,

  $$\frac{\partial F}{\partial x}+\frac{\partial F}{\partial y}\frac{\partial y}{\partial x}=0$$
\end{dft}\n

In other words,

$$\frac{\partial y}{\partial x}=-\frac{\frac{\partial F}{\partial x}}{\frac{\partial F}{\partial y}}$$\s

Here partial derivatives are used. The partial derivative $\frac{\partial F}{\partial x}$ is the derivative of $F$ with respect of $x$, considering $y$ as constant. Note that in the above case, if $\frac{\partial F}{\partial y}\neq 0$, then the implicit function can be rearranged to an explicit function with $y$ as subject.

\subsection{Second and Higher Derivatives}
\subsubsection{Second Derivatives}
\begin{dft}
  Denote a second differentiable function as $y=f(x)$. The \textbf{second derivative} of $f(x)$ is the function

  $$\frac{\diff^2 y}{\diff^2 x}=\frac{\diff}{\diff x}\brr{\frac{\diff y}{\diff x}}$$
\end{dft}\n

Note that the second derivative of $f(x)$ can also be denoted as $f''(x)$ or $y''$.

\subsubsection{Higher Derivatives}
\begin{dft}
  Denote a non-negative integer as $n$. The \textbf{n-th derivative} of $f(x)$ is defined inductively by

  $$\begin{aligned}[t]
    \frac{\diff^{n} y}{\diff x^{n}}&=\frac{\diff}{\diff x}\brr{\frac{\diff^{n-1} y}{\diff x^{n-1}}}\\
    \frac{\diff^{0} y}{\diff x^{0}}&=y
  \end{aligned}$$
\end{dft}\n

Note that the n-th derivative of $f(x)$ can also be denoted as $f^{(n)}(x)$ or $y^{(n)}$. Also note that $f^{(0)}(x)=f(x)$.

\subsubsection{Leinbiz's Rule}
\begin{thm}
  Denote differentiable functions of $x$ be $u$ and $v$. Then by \textbf{Leinbiz's Rule},

  $$(uv)^{(n)}=\sum_{k=0}^{n}\binom{n}{k}u^{(n-k)}v^{(k)}$$\s

  where $\binom{n}{k}=\frac{n!}{k!(n-k)!}$ is the binomial coefficient.\n

  \prf When $n=0$, $(uv)^{(0)}=uv=u^{(0)}v^{(0)}$.\n

  Assume that for some non-negative integer $m$,

  $$(uv)^{(m)}=\sum_{k=0}^{m}\binom{m}{k}u^{(m-k)}v^{(k)}$$\s

  Then

  $$\begin{aligned}[t]
    (uv)^{(m+1)}&=\frac{\diff}{\diff x}(uv)^{(m)}\\
    &=\frac{\diff}{\diff x}\sum_{k=0}^{m}\binom{m}{k}u^{(m-k)}v^{(k)}\\
    &=\sum_{k=0}^{m}\binom{m}{k}(u^{(m-k+1)}v^{(k)}+u^{(m-k)}v^{(k+1)})\\
    &=\sum_{k=0}^{m}\binom{m}{k}u^{(m-k+1)}v^{(k)}+\sum_{k=0}^{m}\binom{m}{k}u^{(m-k)}v^{(k+1)}\\
    &=\sum_{k=0}^{m}\binom{m}{k}u^{(m-k+1)}v^{(k)}+\sum_{k=1}^{m+1}\binom{m}{k-1}u^{(m-(k-1))}v^{(k)}\\
    &=\sum_{k=0}^{m}\binom{m}{k}u^{(m-k+1)}v^{(k)}+\sum_{k=1}^{m+1}\binom{m}{k-1}u^{(m-k+1)}v^{(k)}
  \end{aligned}$$\s

  Note that $\binom{m}{-1}=\binom{m}{m+1}=0$, then

  $$\begin{aligned}[t]
    (uv)^{(m+1)}&=\sum_{k=0}^{m}\binom{m}{k}u^{(m-k+1)}v^{(k)}+\sum_{k=1}^{m+1}\binom{m}{k-1}u^{(m-k+1)}v^{(k)}\\
    &=\sum_{k=0}^{m+1}\brr{\binom{m}{k}+\binom{m}{k-1}}u^{(m-k+1)}v^{(k)}\\
    &=\sum_{k=0}^{m+1}\binom{m+1}{k}u^{(m-k+1)}v^{(k)}
  \end{aligned}$$\s

  Substitute $m+1$ with $n$ and the summation will be the same as above, thus finishes the proof.
\end{thm}

\subsection{Mean Value Theorem}
\subsubsection{General Theorem}
\begin{thm}
  Denote a function on $(a,b)$ as $f$ and a real number in $(a,b)$ as $c$ such that the following applies:

  \begin{alist}
    \item $f$ is differentiable at $x=c$.

    \item Either $f(x)\leq f(c)$ or $f(x)\geq f(c)$ for any $x\in (a,b)$.
  \end{alist}

  Then by \textbf{Mean Value Theorem}, $f'(c)=0$.\n

  \prf Suppose $f(x)\leq f(c)$ for any $x\in (a,b)$. For any $h<0$ with $a<c+h<c$, $f(c+h)-f(c)\leq 0$ and $h$ is negative. Then

  $$f'(c)=\lim_{h\to 0^{-}}\frac{f(c+h)-f(c)}{h}\geq 0$$\s

  On the other hand, for any $h>0$ with $c<c+h<b$, $f(c+h)-f(c)\leq 0$ and $h$ is positive. Then

  $$f'(c)=\lim_{h\to 0^{+}}\frac{f(c+h)-f(c)}{h}\leq 0$$\s

  Therefore $f'(c)=0$. The proof for $f(x)\geq f(c)$ is essentially the same, thus completing the general proof.
\end{thm}

\subsubsection{Rolle's Theorem}
\begin{thm}
  Denote a functon as $f(x)$. Suppose $f(x)$ satisfies the following conditions:

  \begin{alist}
    \item $f(x)$ is continuous on $[a,b]$.

    \item $f(x)$ is differentiable on $(a,b)$.

    \item $f(a)$ is equal to $f(b)$.
  \end{alist}

  Then by \textbf{Rolle's Theorem}, there exists $\xi\in(a,b)$ such that $f'(\xi)=0$.\n

  \prf By Extreme Value Theorem, there exist $\alpha\in[a,b]$ and $\beta\in[a,b]$ such that

  $$f(\alpha)\leq f(x)\leq f(\beta)\erm{for any }x\in [a,b]$$\s

  Since $f(a)=f(b)$, at least one of $\alpha$, $\beta$ can be chosen in $(a,b)$ as $\xi$. Then $f'(\xi)=0$ since $f(x)$ attains its maximum or minimum at $\xi$.
\end{thm}

\subsubsection{Lagrange's Mean Value Theorem}
\begin{thm}
  Denote a function as $f(x)$. Suppose $f(x)$ satisfies the following conditions:

  \begin{alist}
    \item $f(x)$ is continuous on $[a,b]$.

    \item $f(x)$ is differentiable on $(a,b)$.
  \end{alist}

  Then by \textbf{Lagrange's Mean Value Theorem}, there exist $\xi\in [a,b]$ such that

  $$f'(\xi)=\frac{f(b)-f(a)}{b-a}$$\s

  In fact Lagrange's Mean Value Theorem is the generalized version of Rolle's Theorem. Below is the proof of Lagrange's Mean Value Theorem, with the help of Rolle's Theorem:\n

  \prf Let $g(x)=f(x)-\frac{f(b)-f(a)}{b-a}(x-a)$. Since $g(a)=g(b)=f(a)$, by Rolle's Theorem, there exists $\xi\in (a,b)$ such that

  $$\begin{aligned}[t]
    g'(\xi)&=0\\
    f'(\xi)-\frac{f(b)-f(a)}{b-a}&=0\\
    f'(\xi)&=\frac{f(b)-f(a)}{b-a}
  \end{aligned}$$\s
\end{thm}

Lagrange's Mean Value Theorem is one of the most important results in real analysis. Here are some theorems based on Lagrange's Mean Value Theorem.\n

\begin{thm}
  Denote a differentiable function on $(a,b)$ as $f(x)$. Then $f(x)$ is monotonic increasing if and only if $f'(x)\geq 0$ for any $x\in (a,b)$.

  \prf\arr Suppose $f(x)$ is monotonic increasing on $(a,b)$. Then for any $x\in (a,b)$, we have $f(x+h)-f(x)\geq 0$ for any $h>0$ and thus

  $$f'(x)=\lim_{h\to 0^{+}}\frac{f(x+h)-f(x)}{h}\geq 0$$\s

  \arl Suppose $f'(x)\geq 0$ for any $x\in (a,b)$. Then for any $\alpha ,\beta\in (a,b)$ with $\alpha<\beta$, by Lagrange's Mean Value Theorem, there exists $\xi\in (a,b)$ such that

  $$f(\beta)-f(\alpha)=f'(\xi)(\beta-\alpha)\geq 0$$\s

  Therefore $f(x)$ is monotonic increasing on $(a,b)$.
\end{thm}\n

\begin{crl}
  Denote a differentiable function on $(a,b)$ as $f(x)$. $f(x)$ is constant on $(a,b)$ if and only if $f'(x)=0$ for any $x\in(a,b)$.
\end{crl}\n

A similar theorem of a strictly increasing function can be proved with similar method as above. However, it is only correct that if the first derivative of the function is greater than zero, then the function is strictly increasing. Its converse is not true.

\subsubsection{Cauchy's Mean Value Theorem}
\begin{thm}
  Denote functions as $f(x)$ and $g(x)$, where both of the functions satisfy the following conditions:

  \begin{alist}
    \item $f(x),g(x)$ is continuous on $[a,b]$.

    \item $f(x),g(x)$ is differentiable on $(a,b)$.

    \item $g'(x)\neq 0$ for any $x\in (a,b)$.
  \end{alist}

  Then by \textbf{Cauchy's Mean Value Theorem}, there exists $\xi\in (a,b)$ such that

  $$\frac{f'(\xi)}{g'(\xi)}=\frac{f(b)-f(a)}{g(b)-g(a)}$$
\end{thm}

\subsection{L'Hopital's Rule}
\subsubsection{General Rule}
\begin{thm}
  Let $a\in[-\infty,\infty]$. Denote two differentiable functions as $f$ and $g$. Suppose both of the functions satisfy the following requirements:

  \begin{alist}
    \item $\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=0$ (or $\pm\infty$).

    \item $g'(x)\neq 0$ for any $x\neq a$ (on a neighborhood of $a$).

    \item $\lim_{x\to a}\frac{f'(x)}{g'(x)}=L$.
  \end{alist}

  Then by \textbf{L'Hopital's Rule}, the limit of $\frac{f(x)}{g(x)}$ at $x=a$ exists and $\lim_{x\to a}\frac{f(x)}{g(x)}=L$.\n

  \prf Let $a\in (-\infty,\infty)$. For any $x\neq a$, by applying Cauchy's Mean Value Theorem to $f(x)$ and $g(x)$ on $[a,x]$ or $[x,a]$, there exists some $\xi$ between $a$ and $x$ such that

  $$\frac{f'(\xi)}{g'(\xi)}=\frac{f(x)-f(a)}{g(x)-g(a)}=\frac{f(x)}{g(x)}$$\s

  In order to ensure that $f(x)$ and $g(x)$ are both continuous at $a$, it is redefined that $f(a)=g(a)=0$, if necessary. Now note that $\xi\to a$ as $x\to a$, thus

  $$\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(\xi)}{g'(\xi)}=L$$
\end{thm}

\subsection{Introduction to Taylor Series}
\subsubsection{Taylor Polynomials}
\begin{dft}
  Denote a function as $f(x)$ such that the n-th derivative exists at $x=a$. The \textbf{Taylor polynomial} of degree n of $f(x)$ at $x=a$ is the polynomial

  $$f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^{2}+\frac{f^{(3)}(a)}{3!}(x-a)^{3}+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^{n}$$
\end{dft}

\subsubsection{Taylor Series}
\begin{dft}
  Denote a infinitely differentiable function as $f(x)$. The \textbf{Taylor series} of $f(x)$ at $x=a$ is the infinite power series

  $$T(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^{2}+\frac{f^{(3)}(a)}{3!}(x-a)^{3}+\cdots$$
\end{dft}

\subsection{Curve Sketching}
\subsubsection{Checklist for Curve Sketching}
In order to sketch the graph of $y=f(x)$ accurately, the information below are required:

\begin{alist}
  \item \textbf{Domain}: The values of $x$ where $f(x)$ is defined.

  \item \textbf{x-intercepts}: The values of $x$ that fulfills $f(x)=0$.

  \item \textbf{y-intercept}: The value of $f(0)$.

  \item \textbf{Horizontal asymptotes}: If $\lim_{x\to-\infty/+\infty}f(x)=b$, then $y=b$ is a horizontal asymptote.

  \item \textbf{Vertical asymptotes}  If $\lim_{x\to a^{-}/a^{+}}f(x)=-\infty/+\infty$, then $x=a$ is a vertical asymptote.
\end{alist}

\subsubsection{Oblique Asymptotes}
\begin{dft}
  If $\lim_{x\to-\infty/+\infty}(f(x)-(ax+b))=0$, then $y=ax+b$ is said to be an \textbf{oblique asymptote} of $y=f(x)$.
\end{dft}

\propdisp

\subsubsection{First Derivative Test and Turning Points}
\begin{dft}
  Denote a continuous function as $f(x)$. $f(x)$ is said to have a \textbf{local maximum} at $x=a$ if there exists $\delta>0$ such that $f(x)\leq f(a)$ for any $x\in (a-\delta,a+\delta)$, and a \textbf{local minimum} at $x=a$ if there exists $\delta>0$ such that $f(x)\geq f(a)$ for any $x\in (a-\delta,a+\delta)$.
\end{dft}\n

In some cases, local maximum or local minimum cannot be found explicitly. The \textbf{first derivative test} is a useful tool to check the existence of local maximum and local minimum.\n

\begin{pst}
  Denote a continuous function as $f(x)$. Suppose $f(x)$ has a local maximum or local minimum at $x=a$, then either one of the following is true:

  \begin{alist}
    \item $f'(a)=0$.

    \item $f'(x)$ does not exist at $x=a$.
  \end{alist}
\end{pst}\n

\begin{pst}
  Denote a continuous function as $f(x)$. Suppose $f'(a)=0$ or $f'(a)$ does not exist.\n

   If there exists $\delta>0$ such that $f'(x)>0$ for $x\in(a-\delta,a)$ and $f'(x)<0$ for $x\in(a,a+\delta)$, then $f(x)$ has a local maximum at $x=a$.\n

   On the other hand, if there exists $\delta>0$ such that $f'(x)<0$ for $x\in(a-\delta,a)$ and $f'(x)>0$ for $x\in(a,a+\delta)$, then $f(x)$ has a local minimum at $x=a$.
\end{pst}\n

\begin{dft}
  With the first derivative test, $f(x)$ is said to have a \textbf{turning point} at $x=a$ if $f'(x)$ changes sign at $x=a$.
\end{dft}

\subsubsection{Second Derivative Test, Concavity and Inflexion Points}
Besides the first derivative test, there is also the \textbf{second derivative test}:\n

\begin{pst}
  Denote a continuous function as $f(x)$. Suppose $f'(a)=0$. $f(x)$ has a local maximum at $x=a$ if $f''(a)<0$, and has a local minimum at $x=a$ if $f''(a)>0$.
\end{pst}\n

\begin{dft}
  With the second derivative test, $f(x)$ is said to be \textbf{concave upward} on $(a,b)$ if $f''(x)>0$ on $(a,b)$ and \textbf{concave downward} on $(a,b)$ if $f''(x)<0$ on $(a,b)$. Also, $f(x)$ has an \textbf{inflexion point} at $x=a$ if $f''(x)$ changes sign at $x=a$.
\end{dft}

\pagebreak

\section{Integration}
\subsection{Indefinite Integrals and Substitutions}
\subsubsection{Anti-derivatives and Indefinite Integrals}
\begin{dft}
  Denote a continuous function as $f(x)$. An \textbf{anti-derivative} of $f(x)$ is a function $F(x)$ such that

  $$F'(x)=f(x)$$\s

  Note that anti-derivative has another name of primitive function. The collection of all anti-derivatives of $f(x)$ is called the \textbf{indefinite integral} of $f(x)$, and it is denoted by

  $$\int f(x)\diff x$$\s

  where the function $f(x)$ is called the \textbf{integrand} of the integral.\n

  Also note that anti-derivative of a function is not unique. If $F(x)$ is an anti-derivative of $f(x)$, then $F(x)+C$ is also an anti-derivative of $f(x)$ for any constant $C$. Moreover, any derivative of $f(x)$ is of the form $F(x)+C$, in which

  $$\int f(x)\diff x=F(x)+C$$\s

  where $C$ is an arbitrary constant called the \textbf{integration constant}. Note that $\int f(x)\diff x$ is not a single function but a collection of functions.
\end{dft}

\subsubsection{Rules of Indefinite Integration}
\begin{pst}
  Denote continuous functions as $f(x)$ and $g(x)$. Also denote a real number as $k$. Then the following equations can be applied:

  \begin{alist}
    \item \textbf{Addition of indefinite integrals}

    $$\int(f(x)+g(x))\diff x=\int f(x)\diff x+\int g(x)\diff x$$

    \item \textbf{Scalar Multiplication of First Derivative}

    $$\int kf(x)\diff x=k\int f(x)\diff x$$
  \end{alist}
\end{pst}

\subsubsection{Basic Formulae of Indefinite Integration}
\begin{pst}
  Here are some basic formulae of indefinite integration:

  \begin{alist}
    \item \textbf{Integration of Monomials}

    $$\int x^{n}\diff x=\frac{x^{n+1}}{n+1}+C\erm{for }n\in\R\setminus\brc{1}$$

    \item \textbf{Integration of Exponential Function}

    $$\int e^{x}\diff x=e^{x}+C$$

    \item \textbf{Integration about Logarithmic Function}

    $$\int\frac{1}{x}\diff x=\ln\abs{x}+C$$

    \item \textbf{Integration about Sine Function}

    $$\int\cos(x)\diff x=\sin(x)+C$$

    \item \textbf{Integration about Cosine Function}

    $$\int\sin(x)\diff x=-\cos(x)+C$$

    \item \textbf{Integration about Tangent Function}

    $$\int\sec^{2}(x)\diff x=\tan(x)+C$$

    \item \textbf{Integration about Secant Function}

    $$\int\sec(x)\tan(x)\diff x=\sec(x)+C$$

    \item \textbf{Integration about Cosecant Function}

    $$\int\csc(x)\cot(x)\diff x=-\csc(x)+C$$

    \item \textbf{Integration about Cotangent Function}

    $$\int\csc^{2}(x)\diff x=-\cot(x)+C$$
  \end{alist}
\end{pst}

\subsubsection{Substitutions in Indefinite Integrals}
\begin{thm}
  Denote a continuous function defined on $[a,b]$ as $f(x)$. Suppose there exists a differentiable function $u=\varphi(x)$ and continuous function $g(u)$ such that $f(x)=g(\varphi(x))\varphi'(x)$ for any $x\in(a,b)$, then

  $$\begin{aligned}[t]
    \int f(x)\diff x&=\int g(\varphi(x))\varphi'(x)\diff x\\
    &=\int g(u)\diff u
  \end{aligned}$$
\end{thm}

\subsection{Definite Integrals}
\subsubsection{Partitions}
\begin{dft}
  Denote a function defined on $[a,b]$ as $f(x)$. A \textbf{partition} of $[a,b]$ is a set of finite points

  $$P=\brc{a=x_{0}<x_{1}<x_{2}<\cdots<x_{n}=b}$$

  and it is defined that

  $$\begin{aligned}[t]
    &\Delta x_{k}=x_{k}-x_{k-1}\erm{for }k=1,2,\cdots,n\\
    &\abs{P}=\underset{1\leq k\leq n}{\max}\brc{\Delta x_{k}}
  \end{aligned}$$
\end{dft}

\subsubsection{Riemann Sums and Riemann Integrals}
\begin{dft}
  Denote a function defined on $[a,b]$ as $f(x)$. The \textbf{lower Riemann sum} with respect to the partition $P$ is

  $$L(f,P)=\sum_{k=1}^{n}m_{k}\Delta x_{k}$$\s

  where $m_{k}=\inf\brc{f(x):x_{k-1}\leq x\leq x_{k}}$. On the other hand, the \textbf{upper Riemann sum} with respect to the partition $P$ is

  $$U(f,P)=\sum_{k=1}^{n}M_{k}\Delta x_{k}$$\s

  where $M_{k}=\sup\brc{f(x):x_{k-1}\leq x\leq x_{k}}$.\n
\end{dft}

\begin{dft}
  Denote a closed and bounded interval as $[a,b]$ and a real-valued function defined on $[a,b]$ as $f: [a,b]\to\R$. $f(x)$ is \textbf{Riemann integrable} on $[a,b]$ if the limits of $L(f,P)$ and $U(f,P)$ exist as $\abs{P}$ tends to $0$ and are equal. In this case, the \textbf{Riemann integral} of $f(x)$ over $[a,b]$ is defined as

  $$\int_{a}^{b}f(x)\diff x=\lim_{\abs{P}\to 0}L(f,P)=\lim_{\abs{P}\to 0}U(f,P)$$\s

  Note that $\lim_{\abs{P}\to 0}L(f,P)=L$ if for any $\epsilon>0$, there exists some $\delta=\delta(\epsilon)>0$ such that if $\abs{P}<\delta$, then $\abs{L(f,P)-L}<\epsilon$.
\end{dft}

\subsubsection{Rules of Definite Integration}
\begin{pst}
  Now suppose $f(x)$ and $g(x)$ are integrable functions on $[a,b]$, a real number $c$ where $a<c<b$ and a constant $k$, then the following rules of definite integration can be applied:

  \begin{alist}
    \item \textbf{Addition of Definite Integrals}

    $$\int_{a}^{b}(f(x)+g(x))\diff x=\int_{a}^{b}f(x)\diff x+\int_{a}^{b}g(x)\diff x$$

    \item \textbf{Scalar Multiplication of Definite Integrals}

    $$\int_{a}^{b}kf(x)\diff x=k\int_{a}^{b}f(x)\diff x$$

    \item \textbf{Adjacent Intervals of Definite Integrals}

    $$\int_{a}^{b}f(x)\diff x=\int_{a}^{c}f(x)\diff x+\int_{c}^{b}f(x)\diff x$$

    \item \textbf{Inverted Intervals of Definite Integrals}

    $$\int_{a}^{b}f(x)\diff x=-\int_{b}^{a}f(x)\diff x$$
  \end{alist}
\end{pst}

\subsubsection{Riemann Integrability}
\begin{thm}
  Denote a continuous function on $[a,b]$ be $f(x)$. $f(x)$ is Riemann integrable on $[a,b]$ and we have

  $$\begin{aligned}[t]
    \int_{a}^{b}f(x)\diff x&=\lim_{n\to\infty}\sum_{k=1}^{n}f(x_{k})\Delta x_{k}\\
    &=\lim_{n\to\infty}\sum_{k=1}^{n}f\brr{a+\frac{k}{n}(b-a)}\brr{\frac{b-a}{n}}
  \end{aligned}$$
\end{thm}

\propdisp

\subsection{Introduction to Fundamental Theorem of Calculus}
\subsubsection{Fundamental Theorem of Calculus}
\begin{thm}
  Here the \textbf{Fundamental Theorem of Calculus} is introduced in two parts:

  \begin{alist}
    \item Denote a continuous function on $[a,b]$ be $f(x)$. Let $F:[a,b]\to\R$ be the function defined by

    $$F(x)=\int_{a}^{x}f(t)\diff t$$\s

    Then $F(x)$ is continuous on $[a,b]$, differentiable on $(a,b)$ and

    $$F'(x)=f(x)\erm{for any }x\in(a,b)$$\s

    In other words, the above represents

    $$\frac{\diff}{\diff x}\int_{a}^{x}f(t)\diff t=f(x)\erm{for any }x\in(a,b)$$

    \item Denote a continuous function on $[a,b]$ be $f(x)$. Let $F(x)$ be the primitive function of $f(x)$, or in other words, $F(x)$ is a continuous function on $[a,b]$ and $F'(x)=f(x)$ for any $x\in(a,b)$. Then

    $$\int_{a}^{b}f(x)\diff x=F(b)-F(a)$$
  \end{alist}
\end{thm}

\subsubsection{Limits of Series}
The Fundamental Theorem of Calculus can be used to evaluate limits of series of a certain form:\n

\begin{pst}
  Let $f$ be an infinite series, then

  $$\lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^{n}f\brr{\frac{k}{n}}=\int_{0}^{1}f(x)\diff x$$
\end{pst}

\subsubsection{Derivatives of Functions Defined by Integrals}
\begin{pst}
  Below are the formulae for derivatives of functions defined by integrals:

  \begin{alist}
    \item

    $$\frac{\diff}{\diff x}\int_{a}^{x}f(t)\diff t=f(x)$$

    \item

    $$\frac{\diff}{\diff x}\int_{x}^{b}f(t)\diff t=-f(x)$$

    \item

    $$\frac{\diff}{\diff x}\int_{a}^{v(x)}f(t)\diff t=f(v)\frac{\diff v}{\diff x}$$

    \item

    $$\frac{\diff}{\diff x}\int_{u(x)}^{v(x)}f(t)\diff t=f(v)\frac{\diff v}{\diff x}-f(u)\frac{\diff u}{\diff x}$$
  \end{alist}
\end{pst}

\subsection{Trigonometric Identities}
\subsubsection{General Trigonometric Identities}
\begin{pst}
  Below is the list of useful identities for trigonometric integrals:

  \begin{alist}
    \item

    \begin{rlist}
      \item

      $$\cos^{2}x+\sin^{2}x=1$$

      \item

      $$\sec^{2}x=1+\tan^{2}x$$

      \item

      $$\csc^{2}x=1+\cot^{2}x$$
    \end{rlist}

    \item

    \begin{rlist}
      \item

      $$\cos^{2}x=\frac{1+\cos(2x)}{2}$$

      \item

      $$\sin^{2}x=\frac{1-\cos(2x)}{2}$$

      \item

      $$\sin(x)\cos(x)=\frac{\sin(2x)}{2}$$
    \end{rlist}

    \item

    \begin{rlist}
      \item

      $$\cos(x)\cos(y)=\frac{1}{2}(\cos(x+y)+\cos(x-y))$$

      \item

      $$\cos(x)\sin(y)=\frac{1}{2}(\sin(x+y)-\sin(x-y))$$

      \item

      $$\sin(x)\sin(y)=\frac{1}{2}(\cos(x-y)-\cos(x+y))$$
    \end{rlist}

  \end{alist}

\end{pst}

\subsubsection{Products of Sine and Cosine Functions}
In order to evaluate

$$\int\cos^{m}x\;\sin^{n}x\diff x$$\s

where $m,n$ are non-negative integers, then apply the following by considering the corresponding case:

\begin{alist}
  \item \textbf{If m is odd}\n

  Apply $\cos(x)\diff x=\diff\sin(x)$ (or substitute $u=\sin(x)$).

  \item \textbf{If n is odd}\n

  Apply $\sin(x)\diff x=-\diff\cos(x)$ (or substitute $u=\cos(x)$).

  \item \textbf{If m and n are both even}\n

  Apply double-angle formulae to reduce the power (in part (b) of general trigonometric identities).
\end{alist}

\subsubsection{Integrals of Trigonometric Functions With Logarithmic Functions}
Here is the list of integrals of trigonometric functions involving logarithmic functions:

\begin{alist}
  \item \textbf{Tangent Function}\n

  $$\int\tan(x)\diff x=\ln\abs{\sec(x)}+C$$

  \item \textbf{Cotangent Function}\n

  $$\int\cot(x)\diff x=\ln\abs{\sin(x)}+C$$

  \item \textbf{Secant function}\n

  $$\int\sec(x)\diff x=\ln\abs{\sec(x)+\tan(x)}+C$$

  \item \textbf{Cosecant function}\n

  $$\int\csc(x)\diff x=\ln\abs{\csc(x)-\cot(x)}+C$$
\end{alist}

\subsubsection{Products of Secant and Tangent Functions}
In order to evaluate

$$\int\sec^{m}x\;\sin^{n}x\diff x$$\s

where $m,n$ are non-negative integers, then apply the following by considering the corresponding case:

\begin{alist}
  \item \textbf{If m is even}\n

  Apply $\sec^{2}x\diff x=\diff\tan(x)$ (or substitute $u=\tan(x)$).

  \item \textbf{If n is odd}\n

  Apply $\sec(x)\tan(x)\diff x=\diff\sec(x)$ (or substitute $u=\sec(x)$).

  \item \textbf{If m is odd and n is even}\n

  Apply $\tan^{2}x=\sec^{2}x-1$ to express the expression in terms of $\sec(x)$ completely.
\end{alist}

\subsection{Handling Higher Degrees of Integrals}
\subsubsection{Integration by Parts}
Suppose the integrand is in the form $uv'$, then it is possible to evaluate the integration by using the following formula

$$\int uv'\diff x=uv-\int u'v\diff x$$

The above method is called \textbf{integration by parts}, and its general form is

$$\int u\diff v=uv-\int v\diff u$$

\subsubsection{Reduction Formula}
Denote an integral with power constant $n$ as $I_{n}$. If it is possible to express $I_{n}$ in terms of $I_{k}$ where $k<n$, then such formula is called \textbf{reduction formula}.

\subsection{Integration of Rational Integrals}
\subsubsection{Rational Functions}
\begin{dft}
  A \textbf{rational function} is a function of the form

  $$R(x)=\frac{f(x)}{g(x)}$$

  where $f(x),g(x)$ are polynomials with real coefficients with $g(x)\neq 0$.
\end{dft}

\subsubsection{Partial Fractions}
\begin{dft}
  Denote a rational function as $R(x)$, then the partial fraction decomposition of $R(x)$ is

  $$R(x)=q(x)+\sum\frac{A}{(x-\alpha)^{k}}+\sum\frac{B(x+\beta)}{((x-\beta)^{2}+\gamma^{2})^{k}}+\sum\frac{C}{((x-\beta)^{2}+\gamma^{2})^{k}}$$\s

  where $q(x)$ is a polynomial, $A,B,C,\alpha,\beta,\gamma$ are real numbers and $k$ is a positive integer.
\end{dft}\n

Sometimes rational functions are difficult to integrate explicitly, and finding the \textbf{partial fraction} of the function can make the integration process easier.\n

\begin{thm}
  Let $R(x)=\frac{f(x)}{g(x)}$ be a rational function. Assume that the leading coefficient of $g(x)$ is $1$, then the following are true:

  \begin{alist}
    \item \textbf{Division Algorithm for Polynomials}\n

    There exists some polynomials $q(x),r(x)$ where $\deg(r(x))<\deg(q(x))$ or $r(x)=0$ such that

    $$R(x)=q(x)+\frac{r(x)}{g(x)}$$\s

    Note that $q(x)$ and $r(x)$ are the quotient and the remainder of the division $f(x)$ by $g(x)$ respectively.

    \item \textbf{Fundamental Theorem of Algebra for Real Polynomials}\n

    The polynomial $g(x)$ can be written as a product of linear or quadratic polynomials. More precisely, there exist some real numbers $\alpha_{1},\cdots,\alpha_{m},\beta_{1},\cdots,\beta_{n},\gamma_{1},\cdots,\gamma_{n}$ and positive integers $k_{1},\cdots,k_{m},l_{1},\cdots,l_{n}$ such that

    $$g(x)=(x-\alpha_{1})^{k_{1}}\cdots(x-\alpha_{m})^{k_{m}}((x+\beta_{1})^{2}+\gamma_{1}^{2})^{l_{1}}\cdots((x+\beta_{n})^{2}+\gamma_{n}^{2})^{l_{n}}$$
  \end{alist}
\end{thm}\n

\begin{thm}
  Suppose $\frac{f(x)}{g(x)}$ is a rational function such that the degree of $f(x)$ is smaller than that of $g(x)$ and $g(x)$ has only simple roots, which is

  $$g(x)=a(x-\alpha_{1})(x-\alpha_{2})\cdots(x-\alpha_{k})$$\s

  for distinct real numbers $\alpha_{1},\alpha_{2},\cdots,\alpha_{k}$ and $a\neq 0$, then

  $$\frac{f(x)}{g(x)}=\sum_{i=1}^{k}\frac{f(\alpha_{i})}{g'(\alpha_{i})(x-\alpha_{i})}$$
\end{thm}

\subsection{Trigonometric Substitutions}
\subsubsection{Common Trigonometric Substitutions}
Here is a list of common trigonometric substitutions:

\begin{alist}
  \item $\sqrt{k^{2}-x^{2}}$\s

  Apply $\diff x=k\cos(\theta)\diff\theta$ (or $x=k\sin(\theta)$), where

  $$\sin(\theta)=\frac{x}{k},\;\cos(\theta)=\frac{\sqrt{k^{2}-x^{2}}}{k},\;\tan(\theta)=\frac{x}{\sqrt{k^{2}-x^{2}}}$$

  \item $\sqrt{a^{2}+x^{2}}$\s

  Apply $\diff x=k\sec^{2}(\theta)\diff\theta$ (or $x=k\tan(\theta)$), where

  $$\sin(\theta)=\frac{x}{\sqrt{a^{2}+x^{2}}},\;\cos(\theta)=\frac{k}{\sqrt{a^{2}+x^{2}}},\;\tan(\theta)=\frac{x}{k}$$

  \item $\sqrt{x^{2}-k^{2}}$\s

  Apply $\diff x=k\sec(\theta)\tan(\theta)\diff\theta$ (or $x=k\sec(\theta)$), where

  $$\sin(\theta)=\frac{\sqrt{x^{2}-k^{2}}}{x},\;\cos(\theta)=\frac{k}{x},\;\tan(\theta)=\frac{\sqrt{x^{2}-k^{2}}}{k}$$
\end{alist}

With the above equations, the following formulae is derived:

\begin{alist}
  \item \textbf{Derived Formula of $\sqrt{k^{2}-x^{2}}$}

  $$\int\frac{\diff x}{\sqrt{k^{2}-x^{2}}}=\sin^{-1}\brr{\frac{x}{k}}+C$$

  \item \textbf{Derived Formula of $\sqrt{k^{2}+x^{2}}$}

  $$\int\frac{k\diff x}{k^{2}+x^{2}}=\tan^{-1}\brr{\frac{x}{k}}+C$$

  \item \textbf{Derived Formula of $\sqrt{x^{2}-k^{2}}$}

  $$\int\frac{\diff x}{x\sqrt{x^{2}-k^{2}}}=\sec^{-1}\brr{\frac{x}{k}}+C$$
\end{alist}

\subsubsection{t-substitutions}
In order to evaluate $\int R(\sin(x),\cos(x),\tan(x))\diff x$ where $R$ is a rational function, we may perform the \textbf{t-substitution}, which is

$$t=\tan\brr{\frac{x}{2}}$$\s

and we have

$$\int R(\sin(x),\cos(x),\tan(x))\diff x=\int R\brr{\frac{2t}{1+t^{2}},\frac{1-t^{2}}{1+t^{2}},\frac{2t}{1-t^{2}}}\frac{2\diff t}{1+t^{2}}$$\s

which is an integral of rational function.

\pagebreak

\fancyhead[R]{References}
\section*{References}
\addcontentsline{toc}{section}{References}
The following are the references of the context of this document:

\begin{alist}
  \item George B. Thomas Jr., Maurice D. Weir, Joel R. Hass, \textit{Thomas' Calculus}, Pearson (12th/13th Edition), 2010/2014.
  \item K. E. Hirst, \textit{Calculus of One Variable}, Springer, 2006.
\end{alist}

\pagebreak

\end{document}
